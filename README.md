# LLMStack Open Source Deployment

Complete deployment guide and scripts for running LLMStack with 100% free and open source components. Zero API costs, full data privacy, production-ready system.

## ğŸ¯ Purpose

Deploy LLMStack with local AI models (Ollama, LM Studio) and free AI agents (AutoGen, Flowise, OpenHands, Aider) for a completely self-hosted AI development environment.

## ğŸ“‹ System Requirements

- **CPU:** 4+ cores recommended
- **RAM:** 8GB+ (16GB recommended)
- **Storage:** 50GB+ free disk space
- **GPU:** Optional but recommended (4GB+ VRAM)
- **OS:** Linux, macOS, or Windows with WSL2

## ğŸš€ Quick Start

### Phase 1: System Check
```bash
bash scripts/check_system.sh
```

### Phase 2: Install Local Model Server
```bash
bash scripts/install_ollama.sh
```

### Phase 3: Deploy LLMStack
```bash
bash scripts/deploy_llmstack.sh
```

### Phase 4: Install AI Agents
```bash
bash scripts/install_agents.sh
```

### Phase 5: Validate Deployment
```bash
bash scripts/validate_deployment.sh
```

## ğŸ§  Available AI Agents

- **Ollama** - Local model inference (Llama, Mistral, CodeLlama)
- **AutoGen** - Multi-agent conversations
- **Flowise** - Visual AI workflow builder
- **OpenHands** - AI coding assistant
- **Aider** - AI pair programming

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   LLMStack UI   â”‚    â”‚    Flowise      â”‚    â”‚   OpenHands     â”‚
â”‚  (Port 3000)    â”‚    â”‚  (Port 3001)    â”‚    â”‚  (Port 3002)    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                       â”‚                       â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                 â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚     Ollama      â”‚
                    â”‚  (Port 11434)   â”‚
                    â”‚                 â”‚
                    â”‚ â€¢ llama3.2:3b   â”‚
                    â”‚ â€¢ mistral:7b    â”‚
                    â”‚ â€¢ codellama:7b  â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ“š Directory Structure

```
.
â”œâ”€â”€ scripts/                    # Deployment scripts
â”‚   â”œâ”€â”€ check_system.sh        # System requirements check
â”‚   â”œâ”€â”€ install_ollama.sh      # Ollama installation
â”‚   â”œâ”€â”€ deploy_llmstack.sh     # LLMStack deployment
â”‚   â”œâ”€â”€ install_agents.sh      # AI agents installation
â”‚   â”œâ”€â”€ validate_deployment.sh # Deployment validation
â”‚   â””â”€â”€ orchestrator.py        # Agent orchestration
â”œâ”€â”€ llmstack/                  # LLMStack examples and configs
â”œâ”€â”€ ai-tools/                  # Additional AI tools
â”œâ”€â”€ requirements.txt           # Python dependencies
â””â”€â”€ README.md                  # This file
```

## ğŸ”§ Advanced Usage

### Agent Orchestration
```python
from scripts.orchestrator import FreeAgentOrchestrator

orchestrator = FreeAgentOrchestrator()

# Route requests to appropriate agents
response = await orchestrator.route_request(
    "Write a Python function to calculate fibonacci",
    task_type="code"
)
```

### Custom Model Configuration
Edit `~/.autogen/config.json` to add custom models:
```json
{
  "model_list": [
    {
      "model": "your-custom-model",
      "base_url": "http://localhost:11434/v1",
      "api_key": "ollama",
      "api_type": "openai"
    }
  ]
}
```

## ğŸ›ï¸ Access Points

After successful deployment:

- **LLMStack UI:** http://localhost:3000
- **Flowise:** http://localhost:3001
- **OpenHands:** http://localhost:3002
- **Ollama API:** http://localhost:11434/v1

## ğŸ“Š Cost Breakdown

| Component | Cost | Alternative |
|-----------|------|-------------|
| LLMStack | $0 (Open Source) | Langflow ($99/mo) |
| Local Models | $0 (Self-hosted) | OpenAI GPT-4 ($20/mo) |
| AutoGen | $0 (Open Source) | AgentGPT ($30/mo) |
| Flowise | $0 (Open Source) | Zapier ($20/mo) |
| **Total** | **$0/month** | **$169/month** |

## ğŸ› ï¸ Troubleshooting

### Service won't start
```bash
docker logs <container_name>
sudo systemctl status ollama
lsof -i :3000  # Check port conflicts
```

### Out of memory
```bash
docker system prune -a
ollama rm unused_model
```

### Slow inference
```bash
export OLLAMA_NUM_PARALLEL=1
ollama run llama3.2:1b  # Use smaller model
```

## ğŸ¤ Contributing

1. Fork the repository
2. Create a feature branch
3. Add your improvements
4. Submit a pull request

## ğŸ“„ License

MIT License - see LICENSE file for details.

## ğŸ™ Acknowledgments

- [LLMStack](https://github.com/trypromptly/LLMStack) - Main platform
- [Ollama](https://ollama.ai/) - Local model inference
- [AutoGen](https://github.com/microsoft/autogen) - Multi-agent framework
- [Flowise](https://flowiseai.com/) - Visual AI workflows

---

**Total API Cost: $0.00 â€¢ Data Privacy: 100% Local â€¢ Production Ready: YES**