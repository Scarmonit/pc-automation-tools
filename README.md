# LLMStack Open Source Deployment

Complete deployment guide and scripts for running LLMStack with 100% free and open source components. Zero API costs, full data privacy, production-ready system.

## ğŸ¯ Purpose

Deploy LLMStack with local AI models (Ollama, LM Studio, vLLM) and free AI agents (AutoGen, Flowise, OpenHands, Aider) for a completely self-hosted AI development environment with comprehensive monitoring, optimization, and production applications.

## ğŸ“‹ System Requirements

- **CPU:** 4+ cores recommended
- **RAM:** 8GB+ (16GB recommended)  
- **Storage:** 50GB+ free disk space
- **GPU:** Optional but recommended (4GB+ VRAM)
- **OS:** Linux, macOS, or Windows with WSL2
- **Docker:** Required for containerized services

## ğŸš€ Quick Start

### One-Command Deployment
```bash
bash deploy.sh
```

### Manual Step-by-Step Deployment

#### Phase 1: System Check
```bash
bash scripts/check_system.sh
```

#### Phase 2: Install Local Model Servers
```bash
bash scripts/install_ollama.sh
bash scripts/install_lm_studio.sh
bash scripts/setup_vllm.sh
```

#### Phase 3: Deploy LLMStack
```bash
bash scripts/deploy_llmstack.sh
```

#### Phase 4: Install AI Agents
```bash
bash scripts/install_agents.sh
bash scripts/install_continue.sh
bash scripts/install_jan.sh
```

#### Phase 5: Setup Monitoring & Optimization
```bash
bash scripts/setup_monitoring.sh
bash scripts/optimize_system.sh
```

#### Phase 6: Validate & Benchmark
```bash
bash scripts/validate_deployment.sh
python3 scripts/benchmark_system.py
```

## ğŸ§  Available AI Components

### Local Model Servers
- **Ollama** - Local model inference (Llama, Mistral, CodeLlama)
- **LM Studio** - User-friendly model management
- **vLLM** - High-performance GPU inference

### AI Agents & Tools
- **AutoGen** - Multi-agent conversations
- **Flowise** - Visual AI workflow builder  
- **OpenHands** - AI coding assistant
- **Aider** - AI pair programming
- **Continue** - VS Code AI extension
- **Jan** - Desktop AI assistant

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   LLMStack UI   â”‚    â”‚    Flowise      â”‚    â”‚   OpenHands     â”‚
â”‚  (Port 3000)    â”‚    â”‚  (Port 3001)    â”‚    â”‚  (Port 3002)    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                       â”‚                       â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                 â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚     Ollama      â”‚
                    â”‚  (Port 11434)   â”‚
                    â”‚                 â”‚
                    â”‚ â€¢ llama3.2:3b   â”‚
                    â”‚ â€¢ mistral:7b    â”‚
                    â”‚ â€¢ codellama:7b  â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ“š Directory Structure

```
.
â”œâ”€â”€ deploy.sh                  # One-command deployment
â”œâ”€â”€ scripts/                   # Deployment scripts
â”‚   â”œâ”€â”€ check_system.sh       # System requirements check
â”‚   â”œâ”€â”€ install_ollama.sh     # Ollama installation
â”‚   â”œâ”€â”€ install_lm_studio.sh  # LM Studio installation
â”‚   â”œâ”€â”€ setup_vllm.sh         # vLLM setup
â”‚   â”œâ”€â”€ deploy_llmstack.sh    # LLMStack deployment
â”‚   â”œâ”€â”€ install_agents.sh     # AI agents installation
â”‚   â”œâ”€â”€ install_continue.sh   # VS Code Continue extension
â”‚   â”œâ”€â”€ install_jan.sh        # Jan desktop app
â”‚   â”œâ”€â”€ configure_providers.py # Provider configuration
â”‚   â”œâ”€â”€ setup_monitoring.sh   # Monitoring stack
â”‚   â”œâ”€â”€ optimize_system.sh    # System optimization
â”‚   â”œâ”€â”€ benchmark_system.py   # Performance benchmarking
â”‚   â”œâ”€â”€ manage_services.sh    # Service management
â”‚   â”œâ”€â”€ troubleshoot.sh       # Troubleshooting tool
â”‚   â”œâ”€â”€ validate_deployment.sh # Deployment validation
â”‚   â””â”€â”€ orchestrator.py       # Agent orchestration
â”œâ”€â”€ apps/                     # Production applications
â”‚   â”œâ”€â”€ rag_chatbot.json     # RAG chatbot configuration
â”‚   â””â”€â”€ code_pipeline.sh     # Code generation pipeline
â”œâ”€â”€ llmstack/                 # LLMStack examples and configs
â”œâ”€â”€ ai-tools/                 # Additional AI tools
â”œâ”€â”€ requirements.txt          # Python dependencies
â””â”€â”€ README.md                 # This file
```

## ğŸ”§ Advanced Usage

### Agent Orchestration
```python
from scripts.orchestrator import FreeAgentOrchestrator

orchestrator = FreeAgentOrchestrator()

# Route requests to appropriate agents
response = await orchestrator.route_request(
    "Write a Python function to calculate fibonacci",
    task_type="code"
)
```

### Custom Model Configuration
Edit `~/.autogen/config.json` to add custom models:
```json
{
  "model_list": [
    {
      "model": "your-custom-model",
      "base_url": "http://localhost:11434/v1",
      "api_key": "ollama",
      "api_type": "openai"
    }
  ]
}
```

## ğŸ›ï¸ Access Points

After successful deployment:

### Main Applications
- **LLMStack UI:** http://localhost:3000
- **Flowise:** http://localhost:3001  
- **OpenHands:** http://localhost:3002

### Monitoring & Management
- **Grafana:** http://localhost:3003 (admin/admin)
- **Prometheus:** http://localhost:9090

### API Endpoints  
- **Ollama API:** http://localhost:11434/v1
- **LM Studio API:** http://localhost:1234/v1
- **vLLM API:** http://localhost:8000/v1
- **Jan API:** http://localhost:1337/v1

## ğŸ“Š Cost Breakdown

| Component | Cost | Alternative |
|-----------|------|-------------|
| LLMStack | $0 (Open Source) | Langflow ($99/mo) |
| Local Models | $0 (Self-hosted) | OpenAI GPT-4 ($20/mo) |
| AutoGen | $0 (Open Source) | AgentGPT ($30/mo) |
| Flowise | $0 (Open Source) | Zapier ($20/mo) |
| **Total** | **$0/month** | **$169/month** |

## ğŸ› ï¸ Management Commands

### Service Management
```bash
# Start all services
bash scripts/manage_services.sh start

# Stop all services  
bash scripts/manage_services.sh stop

# Check service status
bash scripts/manage_services.sh status

# View service logs
bash scripts/manage_services.sh logs
```

### Performance & Optimization
```bash
# Run performance benchmark
python3 scripts/benchmark_system.py

# Optimize system settings
bash scripts/optimize_system.sh

# Configure providers (after getting admin token from LLMStack UI)
python3 scripts/configure_providers.py <admin_token>
```

### Troubleshooting
```bash
# Interactive troubleshooting tool
bash scripts/troubleshoot.sh

# Common fixes
docker system prune -a          # Clean up Docker
ollama rm unused_model          # Remove unused models
bash scripts/optimize_system.sh # Apply optimizations
```

### Production Applications
```bash
# Generate code project
bash apps/code_pipeline.sh "my-app" "Create a FastAPI web service"

# RAG chatbot configuration available in apps/rag_chatbot.json
```

## ğŸ¤ Contributing

1. Fork the repository
2. Create a feature branch
3. Add your improvements
4. Submit a pull request

## ğŸ“„ License

MIT License - see LICENSE file for details.

## ğŸ™ Acknowledgments

- [LLMStack](https://github.com/trypromptly/LLMStack) - Main platform
- [Ollama](https://ollama.ai/) - Local model inference
- [AutoGen](https://github.com/microsoft/autogen) - Multi-agent framework
- [Flowise](https://flowiseai.com/) - Visual AI workflows

---

**Total API Cost: $0.00 â€¢ Data Privacy: 100% Local â€¢ Production Ready: YES**