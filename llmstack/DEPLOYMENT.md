# LLMStack Free AI Stack Deployment Guide

## üöÄ Zero API Costs | 100% Local | Production Ready

This deployment guide sets up a complete AI infrastructure with **absolutely no API costs**, running entirely on your local machine with production-grade capabilities.

## üìã What You Get

### Core Components
- **Ollama**: Local LLM server with multiple models
- **LLMStack**: Open-source LLM application platform
- **AutoGen**: Microsoft's multi-agent conversation framework
- **Flowise**: Visual AI workflow builder
- **Aider**: AI pair programming in terminal
- **Chroma**: Vector database for RAG applications
- **Grafana**: Monitoring and observability

### Pre-configured Models (All Free)
- **Llama 3.2 (3B)**: Fast general-purpose model
- **Mistral 7B**: Excellent reasoning capabilities
- **CodeLlama 7B**: Specialized for code generation
- **Qwen 2.5 (3B)**: Multilingual support
- **StarCoder2 (3B)**: Code completion

## üñ•Ô∏è System Requirements

| Component | Minimum | Recommended |
|-----------|---------|-------------|
| CPU | 4 cores | 8+ cores |
| RAM | 8 GB | 16+ GB |
| Storage | 50 GB | 100+ GB |
| GPU | Optional | NVIDIA 4GB+ VRAM |
| OS | Windows 10/11 | Windows 11 |

## ‚ö° Quick Start

### One-Command Deployment

```powershell
# Clone the repository
git clone https://github.com/Scarmonit/pc-automation-tools.git
cd pc-automation-tools

# Run the deployment script
.\deploy.ps1
```

This will:
1. Check system requirements
2. Install Ollama and download models
3. Set up Docker containers
4. Configure AI agents
5. Validate the installation

### Manual Step-by-Step Installation

#### Step 1: Check System Requirements
```powershell
.\deployment\windows\check_system.ps1
```

#### Step 2: Install Ollama and Models
```powershell
.\deployment\windows\install_ollama.ps1
```

#### Step 3: Deploy Docker Services
```powershell
.\deploy.ps1 -Phase docker
```

#### Step 4: Configure AI Agents
```powershell
.\deployment\windows\setup_agents.ps1
```

#### Step 5: Validate Installation
```powershell
.\deployment\windows\validate.ps1
```

## üîß Configuration

### Environment Variables
The deployment creates a `.env` file with all necessary configurations:

```env
POSTGRES_PASSWORD=<auto-generated>
FLOWISE_PASSWORD=<auto-generated>
GRAFANA_PASSWORD=<auto-generated>
OLLAMA_API_BASE=http://localhost:11434/v1
```

### Model Configuration
Models are automatically configured for optimal performance based on your hardware.

## üìö Usage Examples

### 1. Chat with Local LLM
```powershell
ollama run llama3.2:3b
```

### 2. Code Generation with Aider
```powershell
aider --message "Add unit tests to this file" src\myfile.py
```

### 3. Multi-Agent Conversation with AutoGen
```python
python agents\autogen_example.py
```

### 4. API Usage (OpenAI Compatible)
```python
import httpx

response = httpx.post(
    "http://localhost:11434/v1/chat/completions",
    json={
        "model": "llama3.2:3b",
        "messages": [{"role": "user", "content": "Hello!"}]
    }
)
print(response.json())
```

## üåê Access Points

| Service | URL | Credentials |
|---------|-----|-------------|
| Main Application | http://localhost:3000 | See `.env` |
| Flowise UI | http://localhost:3001 | admin / (see `.env`) |
| Monitoring (Grafana) | http://localhost:3003 | admin / (see `.env`) |
| Vector DB (Chroma) | http://localhost:8001 | None |
| Ollama API | http://localhost:11434 | None |

## üõ†Ô∏è Troubleshooting

### Issue: Models download slowly
**Solution**: Models are large (2-4GB each). Initial download requires patience. Use `-SkipModels` flag to skip.

### Issue: Out of memory errors
**Solution**: 
```powershell
# Reduce number of parallel models
$env:OLLAMA_NUM_PARALLEL = 1

# Use smaller models
ollama run llama3.2:1b
```

### Issue: Docker containers won't start
**Solution**:
```powershell
# Reset Docker
docker-compose down -v
docker system prune -a
.\deploy.ps1 -Phase docker
```

### Issue: GPU not detected
**Solution**:
```powershell
# Check CUDA installation
nvidia-smi

# Install CUDA toolkit if missing
# Download from: https://developer.nvidia.com/cuda-downloads
```

## üìä Performance Benchmarks

| Model | First Token (s) | Tokens/sec | RAM Usage |
|-------|-----------------|------------|-----------|
| Llama 3.2 (3B) | 0.8 | 45 | 3 GB |
| Mistral (7B) | 1.2 | 25 | 5 GB |
| CodeLlama (7B) | 1.5 | 20 | 5 GB |

*Benchmarks on RTX 3060 12GB / Ryzen 5 5600X / 32GB RAM*

## üîÑ Updates and Maintenance

### Update Models
```powershell
ollama pull llama3.2:latest
ollama pull mistral:latest
```

### Backup Configuration
```powershell
.\scripts\backup.ps1
```

### Clean Installation
```powershell
.\scripts\clean.ps1
.\deploy.ps1
```

## üìà Monitoring

Access Grafana at http://localhost:3003 to monitor:
- Model inference latency
- Token generation speed
- Memory usage
- Container health
- API request rates

## ü§ù Contributing

Contributions are welcome! Please:
1. Fork the repository
2. Create a feature branch
3. Submit a pull request

## üìù License

MIT License - See LICENSE file

## üÜò Support

- **Issues**: [GitHub Issues](https://github.com/Scarmonit/pc-automation-tools/issues)
- **Documentation**: This file and `/docs` folder
- **Community**: Start a discussion in the repository

## üéØ Next Steps

After successful deployment:

1. **Explore the UI**: Open http://localhost:3001 for Flowise
2. **Test Models**: Try different models with `ollama run <model>`
3. **Build Apps**: Use the APIs to create your own AI applications
4. **Customize**: Modify configurations in `.env` and `docker-compose.yml`

## üí∞ Cost Comparison

| Service | Cloud Cost/Month | This Setup |
|---------|-----------------|------------|
| GPT-4 API | $100-500+ | $0 |
| Claude API | $50-200+ | $0 |
| Vector DB | $50-100+ | $0 |
| Hosting | $20-50+ | $0 |
| **Total** | **$220-850+** | **$0** |

## üèÜ Features Comparison

| Feature | Cloud Services | This Setup |
|---------|---------------|------------|
| Data Privacy | ‚ùå Limited | ‚úÖ 100% Local |
| API Costs | ‚ùå Ongoing | ‚úÖ Zero |
| Internet Required | ‚ùå Always | ‚úÖ Never |
| Customization | ‚ùå Limited | ‚úÖ Full Control |
| Model Selection | ‚ùå Provider Limited | ‚úÖ Any Model |
| Rate Limits | ‚ùå Yes | ‚úÖ None |

---

**Built with ‚ù§Ô∏è for the open-source community**

*Total API Cost: $0.00 | Data Privacy: 100% | Production Ready: YES*